{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Project (Application 1) (MetuSabanci Treebank).conll'\n",
    "\n",
    "sentences = []\n",
    "with open(filename, encoding=\"utf-8-sig\") as infh:\n",
    "    sentence = []\n",
    "    for line in infh:\n",
    "        if len(line) == 1:\n",
    "            sentences.append(sentence.copy())\n",
    "            sentence = []\n",
    "        else:\n",
    "            line_split = line.strip().split('\\t')\n",
    "            if line_split[1] != '_':\n",
    "                if line_split[1].lower() == 'satın': # An error in the corpus that should be fixed by using POS 'Noun'\n",
    "                    sentence.append([line_split[1], 'Noun'])\n",
    "                else:\n",
    "                    sentence.append([line_split[1], line_split[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(sentences)\n",
    "train_size = round(len(sentences)*0.9)\n",
    "training = sentences[:train_size]\n",
    "test = sentences[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train seperate classifiers for the following features\n",
    "# a non-first word whose first letter is capital (e.g. Ahmet -> Noun)\n",
    "def non_first_capital_word_checker(word, **kwargs):\n",
    "    return 'first_token' in kwargs and not kwargs['first_token'] and word[0] == word[0].upper()\n",
    "# ends with -ıp, -ip, -up etc. (e.g. kurtulup, yapıp, edip etc. -> adj)\n",
    "def endswith_up_checker(word, **kwargs):\n",
    "    return re.search(r'[ı,i,u,ü]p$', word) is not None\n",
    "# ends with -rdı, -rdi, -rdik, -rdim etc. (e.g. yapardı, koşardı, getirdik, ettim etc. -> verb)\n",
    "def endswith_rdik_checker(word, **kwargs):\n",
    "    return re.search(r'rd[ı,i,u,ü][m,k]*$', word) is not None\n",
    "# ends with -yı, -yi etc. (e.g. yapmayı, götürüyü etc. -> noun)\n",
    "def endswith_yi_checker(word, **kwargs):\n",
    "    return re.search(r'y[ı,i,u,ü]$', word) is not None\n",
    "# ends with -mış, -miş, -mışsınız, -mişiz etc. (e.g. yapmışız, koşmuşuz etc. -> verb)\n",
    "def endswith_mis_checker(word, **kwargs):\n",
    "    return re.search(r'm[ı,i,u,ü]ş[s]*[ı,i,u,ü]*[n]*[ı,i,u,ü]*[z]*$', word) is not None\n",
    "# ends with -dı, -di, -diler, -tünüz etc. (e.g. yaptık, koştular etc. -> verb)\n",
    "def endswith_di_checker(word, **kwargs):\n",
    "    return re.search(r'[y]*[d,t][ı,i,u,ü][l]*[e,a]*[r]*[k]*[n]*[ı,i,u,ü]*[z]*[m]*$', word) is not None\n",
    "# ends with -ca, -ce etc. (e.g. kısaca, kabaca etc. -> adv)\n",
    "def endswith_ca_checker(word, **kwargs):\n",
    "    return re.search(r'[c,ç][a,e]$', word) is not None\n",
    "# ends with -ecek, -acak etc. (e.g. edeceğiz, yapacaklar etc. -> adv)\n",
    "def endswith_acak_checker(word, **kwargs):\n",
    "    return re.search(r'[a,e]c[a,e][ğ,k][s]*[l]*[a,e,ı,i]*[r]*[n]*[ı,i]*[z]*[m]*$', word) is not None\n",
    "# ends with -an, -en etc. (e.g. yapan, edenleri etc. -> adv)\n",
    "def endswith_an_checker(word, **kwargs):\n",
    "    return re.search(r'[a,e]n[l]*[a,e]*[r]*[ı,i]*$', word) is not None\n",
    "# ends with -e, -a, -de, -da, -den, -ten etc.\n",
    "def endswith_a_checker(word, **kwargs):\n",
    "    return re.search(r'[l]*[a,e]*[r]*[ı,i,u,ü][yl]*[m,n]*[ı,i,u,ü]*[z]*[d,t]*[a,e]*[n]*$', word) is not None\n",
    "# ends with -lerine, -mize etc. (e.g. kendimize, kendilerine etc. -> adv)\n",
    "def endswith_ine_checker(word, **kwargs):\n",
    "    return re.search(r'[l]*[a,e]*[r]*[ı,i]*[m,n][a,e,ı,i][z]*[a,e]*$', word) is not None\n",
    "# ends with -şme, -şma etc. (e.g. sürtüşme, kapışma etc. -> noun)\n",
    "def endswith_sme_checker(word, **kwargs):\n",
    "    return re.search(r'şm[a,e][l]*[a,e]*[r]*[ı,i]*[n]*$', word) is not None\n",
    "\n",
    "features = [non_first_capital_word_checker, endswith_up_checker, endswith_rdik_checker, endswith_yi_checker, \n",
    "            endswith_mis_checker, endswith_di_checker, endswith_ca_checker, endswith_acak_checker, endswith_an_checker,\n",
    "            endswith_a_checker, endswith_ine_checker, endswith_sme_checker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the possible POS tags and Tokens in the training set into two sets\n",
    "POS_tags = set()\n",
    "tokens = set()\n",
    "for sentence in training:\n",
    "    for token, tag in sentence:\n",
    "        POS_tags.add(tag)\n",
    "        tokens.add(token)\n",
    "        \n",
    "# Convert tag and token sets into lists to have reference indices per tag/token\n",
    "POS_tags = list(POS_tags)\n",
    "tokens = list(tokens)\n",
    "\n",
    "# Create and fill the transition and observation probability matrices\n",
    "transition_probs = np.zeros((len(POS_tags)+1, len(POS_tags))) # +1 represents <s> (sentence beginning)\n",
    "observation_probs = np.zeros((len(POS_tags), len(tokens)))\n",
    "t_1_counts = np.zeros(len(POS_tags)+1)\n",
    "t_counts = np.zeros(len(POS_tags))\n",
    "feature_probs = np.zeros((len(POS_tags), len(features)))\n",
    "unknown_probs = np.ones(len(POS_tags)) / 100 # TODO, NO SPECIFIC PRIORS YET!\n",
    "\n",
    "for sentence in training:\n",
    "    last_tag = '<s>'\n",
    "    first_token = True\n",
    "    for token, tag in sentence:\n",
    "        t_1 = 0 if last_tag == '<s>' else POS_tags.index(last_tag) + 1\n",
    "        t = POS_tags.index(tag)\n",
    "        transition_probs[t_1, t] = transition_probs[t_1, t] + 1\n",
    "        t_1_counts[t_1] = t_1_counts[t_1] + 1\n",
    "        t_counts[t] = t_counts[t] + 1\n",
    "        \n",
    "        w = tokens.index(token)\n",
    "        observation_probs[t, w] = observation_probs[t, w] + 1\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature(token, first_token=first_token):\n",
    "                f = features.index(feature)\n",
    "                feature_probs[t, f] = feature_probs[t, f] + 1\n",
    "        \n",
    "        last_tag = tag\n",
    "        \n",
    "        if first_token:\n",
    "            first_token = False\n",
    "        \n",
    "transition_probs = (transition_probs.T/t_1_counts).T\n",
    "observation_probs = (observation_probs.T/t_counts).T\n",
    "feature_probs = (feature_probs.T/t_counts).T\n",
    "unknown_probs = (unknown_probs.T/t_counts).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for the sentences in the test set, generate the most likely POS tag sequence using the\n",
    "Viterbi algorithm. Devise a method for unknown words that is based on morphological and\n",
    "orthographical information. Calculate the success rate of your tagger in terms of the number\n",
    "of words correctly tagged. Also, calculate a sentence-based success rate, which is the ratio of\n",
    "the number of correctly tagged sentences (i.e. all the words in the sentence are tagged\n",
    "correctly) to the total number of sentences. In addition, compute the accuracy of each tag and\n",
    "produce a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_likelihood(state, token, b, first_token=False):\n",
    "    if token in tokens:\n",
    "        return b[state, tokens.index(token)]\n",
    "    w = unknown_probs[state]\n",
    "    for feature in features:\n",
    "        if feature(token, first_token=first_token):\n",
    "            f = features.index(feature)\n",
    "            w = w * feature_probs[state, f]\n",
    "    return w\n",
    "\n",
    "def viterbi(states, observations, a, b):\n",
    "    v = np.zeros((len(states)+2, len(observations)))\n",
    "    backpointer = np.zeros((len(states)+2, len(observations))).astype(np.int)\n",
    "    \n",
    "    for s in range(1,len(states)+1):\n",
    "        v[s,0] = a[0,s-1] * get_token_likelihood(s-1, observations[0], b, first_token=True)\n",
    "        backpointer[s,0] = 0\n",
    "    \n",
    "    for t, o in enumerate(observations):\n",
    "        if t == 0: continue\n",
    "        for s in range(1,len(states)+1):\n",
    "            v_max = -1\n",
    "            v_argmax = -1\n",
    "            bs = get_token_likelihood(s-1, o, b)\n",
    "            for s_prime in range(1,len(states)+1):\n",
    "                v_curr = v[s_prime,t-1] * a[s_prime,s-1] * bs\n",
    "                if v_curr > v_max:\n",
    "                    v_max = v_curr\n",
    "                    v_argmax = s_prime\n",
    "            v[s,t] = v_max\n",
    "            backpointer[s,t] = v_argmax\n",
    "    \n",
    "    v[len(states)+1,len(observations)-1] = np.max(v[:,len(observations)-1])\n",
    "    backpointer[len(states)+1,len(observations)-1] = max(1, np.argmax(v[:,len(observations)-1]))\n",
    "    \n",
    "    trace = []\n",
    "    bp = backpointer[len(states)+1,len(observations)-1]\n",
    "    row = len(states)+1\n",
    "    column = len(observations)-1\n",
    "    while row != 0:\n",
    "        bp = backpointer[row, column]\n",
    "        trace = [bp-1] + trace\n",
    "        if row != len(states)+1:\n",
    "            column = column - 1\n",
    "        row = bp\n",
    "    \n",
    "    return [states[state_idx] for state_idx in trace[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Punc',\n",
       " 'Noun',\n",
       " 'Postp',\n",
       " 'Adv',\n",
       " 'Noun',\n",
       " 'Verb',\n",
       " 'Punc']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi(POS_tags, [token for token, tag in test[2]], transition_probs, observation_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rum', 'Adj'],\n",
       " [\"Kesimi'nin\", 'Noun'],\n",
       " [\"AB'ye\", 'Noun'],\n",
       " ['üye', 'Noun'],\n",
       " ['olması', 'Noun'],\n",
       " ['durumunda', 'Noun'],\n",
       " ['Türkiye', 'Noun'],\n",
       " ['-', 'Punc'],\n",
       " ['AB', 'Noun'],\n",
       " ['ilişkilerinde', 'Noun'],\n",
       " ['sürekli', 'Adv'],\n",
       " ['sürtüşme', 'Noun'],\n",
       " ['yaşanacak', 'Verb'],\n",
       " ['.', 'Punc']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "tag_count = 0\n",
    "for t in range(len(test)):\n",
    "    pred = viterbi(POS_tags, [token for token, tag in test[t]], transition_probs, observation_probs)\n",
    "    gold = list(map(lambda x: x[1], test[t]))\n",
    "    correct += np.sum([1 if gold[i] == pred[i] else 0 for i in range(len(gold))])\n",
    "    tag_count += len(gold)\n",
    "print(correct, tag_count, correct/tag_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a design and implementation document which clearly explains the system. Follow the\n",
    "structure given in http://www.cmpe.boun.edu.tr/~gungort/informationstudents.htm - Graduate\n",
    "Courses “Programming Project Documentation”. Explain the modules, the data structures\n",
    "used, the logic of the algorithms, etc. Show how the system learns the model parameters, the\n",
    "training phase, and the executions on the test data. Include several example test sentences in\n",
    "the document and show clearly the tagging process. The test cases should include\n",
    "correct/incorrect tagging decisions, unknown words, etc. The system should be tested\n",
    "throughly and the test scenarios should be included explicitly in the document.\n",
    "\n",
    "The document will be an important part of the project. The suggested size of the document is\n",
    "about 10-15 pages. Submit the document (with the source code as an appendix) both as hardcopy\n",
    "and via Moodle; submit the program via Moodle. The dealine is 27.11.2018.\n",
    "You will do a demonstration of the project. We will arrange for each group a date and hour\n",
    "for the demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
